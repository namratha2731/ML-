{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oev7GWDsASBu",
    "outputId": "f3e02423-9bae-4cbd-e4e5-81230e6161e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Features file columns: Index(['Unnamed: 0',            0,            1,            2,            3,\n",
      "                  4,            5,            6,            7,            8,\n",
      "       ...\n",
      "                192,          193,          194,          195,          196,\n",
      "                197,          198,          199,        '0.1', 'image_name'],\n",
      "      dtype='object', length=203)\n",
      "Labels file columns: Index(['gt', 'image_name'], dtype='object')\n",
      "Model successfully trained with Baum-Welch algorithm.\n",
      "Predicted sequence for the test features: :\n",
      "Predicted states: [1]\n",
      "State to character mapping: {0: ' ', 1: ':', 2: '\\xa0', 3: 'ം', 4: 'ഃ', 5: 'അ', 6: 'ആ', 7: 'ഇ', 8: 'ഉ', 9: 'എ', 10: 'ഏ', 11: 'ഒ', 12: 'ഓ', 13: 'ക', 14: 'ഖ', 15: 'ഗ', 16: 'ഘ', 17: 'ങ', 18: 'ച', 19: 'ഛ', 20: 'ജ', 21: 'ഝ', 22: 'ഞ', 23: 'ട', 24: 'ഠ', 25: 'ഡ', 26: 'ഢ', 27: 'ണ', 28: 'ത', 29: 'ഥ', 30: 'ദ', 31: 'ധ', 32: 'ന', 33: 'പ', 34: 'ഫ', 35: 'ബ', 36: 'ഭ', 37: 'മ', 38: 'യ', 39: 'ര', 40: 'റ', 41: 'ല', 42: 'ള', 43: 'ഴ', 44: 'വ', 45: 'ശ', 46: 'ഷ', 47: 'സ', 48: 'ഹ', 49: 'ാ', 50: 'ി', 51: 'ീ', 52: 'ു', 53: 'ൂ', 54: 'ൃ', 55: 'െ', 56: 'േ', 57: 'ൈ', 58: 'ൊ', 59: 'ോ', 60: '്', 61: 'ൗ', 62: 'ൻ', 63: 'ർ', 64: 'ൽ', 65: 'ൾ', 66: '\\u200c', 67: '\\u200d'}\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install pandas numpy scikit-learn hmmlearn opencv-python matplotlib openpyxl\n",
    "\n",
    "# Required libraries for handling data and modeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# After uploading, specify the file names for easy reference\n",
    "feature_file = '/content/drive/MyDrive/Colab Notebooks/SFR_DCT_LINE 2.xlsx'  # DCT features file (uploaded manually)\n",
    "label_file = '/content/drive/MyDrive/Colab Notebooks/line_gt_1_1.xlsx'        # Label file (uploaded manually)\n",
    "\n",
    "\n",
    "# Step 2: Load the feature and label data\n",
    "# Load the DCT feature data and label data from provided files\n",
    "features = pd.read_excel(feature_file)\n",
    "labels = pd.read_excel(label_file)\n",
    "\n",
    "# Display column names for both datasets\n",
    "print(\"Features file columns:\", features.columns)\n",
    "print(\"Labels file columns:\", labels.columns)\n",
    "\n",
    "\n",
    "# Step 3: Merge features and labels on 'ImageName' for consistent mapping\n",
    "# This ensures we have corresponding labels for each set of features\n",
    "merged_data = pd.merge(features, labels, on='image_name')\n",
    "merged_data.fillna(0, inplace=True)  # Fill NaN values with zeros\n",
    "\n",
    "# Step 4: Preprocess the data for HMM training\n",
    "# Extract feature matrix (excluding 'ImageName' and 'gt' columns)\n",
    "X = merged_data.drop(columns=['image_name', 'gt']).values\n",
    "\n",
    "# Scale features to mean 0 and variance 1 (to avoid numerical issues in HMM)\n",
    "scaler = StandardScaler()\n",
    "# Step 1: Drop non-numeric columns like 'color' if they exist in the features\n",
    "X = merged_data.select_dtypes(include=[np.number]).values  # Only numeric data\n",
    "\n",
    "# Step 2: Scale features to mean 0 and variance 1 (to avoid numerical issues in HMM)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Map unique characters in ground truth ('gt') to integer values for training\n",
    "unique_chars = sorted(set(''.join(merged_data['gt'].values)))\n",
    "char_to_int = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "int_to_char = {idx: char for char, idx in char_to_int.items()}\n",
    "\n",
    "# Convert 'gt' labels into sequences of integers for training\n",
    "y_sequences = [[char_to_int[char] for char in label] for label in merged_data['gt'].values]\n",
    "\n",
    "# Pad sequences to ensure consistent sequence lengths (fill with -1 for padding)\n",
    "max_sequence_length = max(len(seq) for seq in y_sequences)\n",
    "y_padded = np.array([seq + [-1] * (max_sequence_length - len(seq)) for seq in y_sequences])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y_padded[:train_size], y_padded[train_size:]\n",
    "\n",
    "# Step 6: Initialize and Train the HMM model using Baum-Welch algorithm\n",
    "# Here, we define the HMM model with Gaussian emissions\n",
    "n_states = 6  # Number of hidden states (adjust based on complexity)\n",
    "hmm_model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"spherical\", n_iter=2000, init_params='')\n",
    "\n",
    "# Set start and transition probabilities uniformly\n",
    "hmm_model.startprob_ = np.full(n_states, 1.0 / n_states)\n",
    "hmm_model.transmat_ = np.full((n_states, n_states), 1.0 / n_states)\n",
    "\n",
    "# Baum-Welch algorithm for training the HMM\n",
    "# This will optimize the HMM's parameters (transition and emission probabilities) based on training data\n",
    "try:\n",
    "    hmm_model.fit(X_train)\n",
    "    print(\"Model successfully trained with Baum-Welch algorithm.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# TEST FEATURE PREDICTION PART (using DCT features)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Function to predict character sequences using Viterbi algorithm\n",
    "# The Viterbi algorithm is used here for decoding, providing the most likely sequence of hidden states\n",
    "def predict_from_features(test_features):\n",
    "    if test_features.shape[0] != X.shape[1]:  # Ensure correct number of features\n",
    "        raise ValueError(f\"Expected {X.shape[1]} features, but got {test_features.shape[0]}\")\n",
    "\n",
    "    # Reshape test features to match HMM input requirements\n",
    "    test_features = test_features.reshape(1, -1)\n",
    "\n",
    "    # Viterbi algorithm for decoding the most likely hidden state sequence\n",
    "    # This outputs the sequence of states most likely responsible for the observed sequence\n",
    "    try:\n",
    "        predicted_states = hmm_model.predict(test_features)\n",
    "        predicted_sequence = ''.join(int_to_char[state] for state in predicted_states if state != -1)\n",
    "        print(\"Predicted sequence for the test features:\", predicted_sequence)\n",
    "    except ValueError as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "\n",
    "\n",
    "    print(\"Predicted states:\", predicted_states)\n",
    "\n",
    "    predicted_sequence = ''.join(int_to_char[state] for state in predicted_states if state in int_to_char)\n",
    "\n",
    "# Example: Use random test features (ensure it has the same length as X's feature dimension)\n",
    "# Replace `test_features` with actual data for real predictions\n",
    "test_features = np.random.rand(X.shape[1])  # Random example for testing purposes\n",
    "\n",
    "# Run prediction function to obtain character sequence from test features\n",
    "predict_from_features(test_features)\n",
    "\n",
    "\n",
    "print(\"State to character mapping:\", int_to_char)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BhYSpmLFNuFj",
    "outputId": "c9bc72fe-126f-468a-86d5-93b0a23154af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:hmmlearn.base:Model is not converging.  Current: -14630.897876990844 is not greater than -14630.89787561979. Delta is -1.3710541679756716e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features file columns: Index(['Unnamed: 0',            0,            1,            2,            3,\n",
      "                  4,            5,            6,            7,            8,\n",
      "       ...\n",
      "                192,          193,          194,          195,          196,\n",
      "                197,          198,          199,        '0.1', 'image_name'],\n",
      "      dtype='object', length=203)\n",
      "Labels file columns: Index(['gt', 'image_name'], dtype='object')\n",
      "Model successfully trained with Baum-Welch algorithm.\n",
      "Predicted sequence for the test features: ം\n",
      "Predicted states: [3]\n",
      "State to character mapping: {0: ' ', 1: ':', 2: '\\xa0', 3: 'ം', 4: 'ഃ', 5: 'അ', 6: 'ആ', 7: 'ഇ', 8: 'ഉ', 9: 'എ', 10: 'ഏ', 11: 'ഒ', 12: 'ഓ', 13: 'ക', 14: 'ഖ', 15: 'ഗ', 16: 'ഘ', 17: 'ങ', 18: 'ച', 19: 'ഛ', 20: 'ജ', 21: 'ഝ', 22: 'ഞ', 23: 'ട', 24: 'ഠ', 25: 'ഡ', 26: 'ഢ', 27: 'ണ', 28: 'ത', 29: 'ഥ', 30: 'ദ', 31: 'ധ', 32: 'ന', 33: 'പ', 34: 'ഫ', 35: 'ബ', 36: 'ഭ', 37: 'മ', 38: 'യ', 39: 'ര', 40: 'റ', 41: 'ല', 42: 'ള', 43: 'ഴ', 44: 'വ', 45: 'ശ', 46: 'ഷ', 47: 'സ', 48: 'ഹ', 49: 'ാ', 50: 'ി', 51: 'ീ', 52: 'ു', 53: 'ൂ', 54: 'ൃ', 55: 'െ', 56: 'േ', 57: 'ൈ', 58: 'ൊ', 59: 'ോ', 60: '്', 61: 'ൗ', 62: 'ൻ', 63: 'ർ', 64: 'ൽ', 65: 'ൾ', 66: '\\u200c', 67: '\\u200d'}\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install pandas numpy scikit-learn hmmlearn opencv-python matplotlib openpyxl\n",
    "\n",
    "# Required libraries for handling data and modeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# After uploading, specify the file names for easy reference\n",
    "feature_file = '/content/drive/MyDrive/Colab Notebooks/SFR_DCT_LINE 2.xlsx'  # DCT features file (uploaded manually)\n",
    "label_file = '/content/drive/MyDrive/Colab Notebooks/line_gt_1_1.xlsx'        # Label file (uploaded manually)\n",
    "\n",
    "\n",
    "# Step 2: Load the feature and label data\n",
    "# Load the DCT feature data and label data from provided files\n",
    "features = pd.read_excel(feature_file)\n",
    "labels = pd.read_excel(label_file)\n",
    "\n",
    "# Display column names for both datasets\n",
    "print(\"Features file columns:\", features.columns)\n",
    "print(\"Labels file columns:\", labels.columns)\n",
    "\n",
    "\n",
    "# Step 3: Merge features and labels on 'ImageName' for consistent mapping\n",
    "# This ensures we have corresponding labels for each set of features\n",
    "merged_data = pd.merge(features, labels, on='image_name')\n",
    "merged_data.fillna(0, inplace=True)  # Fill NaN values with zeros\n",
    "\n",
    "# Step 4: Preprocess the data for HMM training\n",
    "# Extract feature matrix (excluding 'ImageName' and 'gt' columns)\n",
    "X = merged_data.drop(columns=['image_name', 'gt']).values\n",
    "\n",
    "# Scale features to mean 0 and variance 1 (to avoid numerical issues in HMM)\n",
    "scaler = StandardScaler()\n",
    "# Step 1: Drop non-numeric columns like 'color' if they exist in the features\n",
    "X = merged_data.select_dtypes(include=[np.number]).values  # Only numeric data\n",
    "\n",
    "# Step 2: Scale features to mean 0 and variance 1 (to avoid numerical issues in HMM)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Map unique characters in ground truth ('gt') to integer values for training\n",
    "unique_chars = sorted(set(''.join(merged_data['gt'].values)))\n",
    "char_to_int = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "int_to_char = {idx: char for char, idx in char_to_int.items()}\n",
    "\n",
    "# Convert 'gt' labels into sequences of integers for training\n",
    "y_sequences = [[char_to_int[char] for char in label] for label in merged_data['gt'].values]\n",
    "\n",
    "# Pad sequences to ensure consistent sequence lengths (fill with -1 for padding)\n",
    "max_sequence_length = max(len(seq) for seq in y_sequences)\n",
    "y_padded = np.array([seq + [-1] * (max_sequence_length - len(seq)) for seq in y_sequences])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y_padded[:train_size], y_padded[train_size:]\n",
    "\n",
    "# Step 6: Initialize and Train the HMM model using Baum-Welch algorithm\n",
    "# Here, we define the HMM model with Gaussian emissions\n",
    "n_states = 6  # Number of hidden states (adjust based on complexity)\n",
    "hmm_model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"spherical\", n_iter=2000, init_params='')\n",
    "\n",
    "# Set start and transition probabilities uniformly\n",
    "hmm_model.startprob_ = np.full(n_states, 1.0 / n_states)\n",
    "hmm_model.transmat_ = np.full((n_states, n_states), 1.0 / n_states)\n",
    "\n",
    "# Baum-Welch algorithm for training the HMM\n",
    "# This will optimize the HMM's parameters (transition and emission probabilities) based on training data\n",
    "try:\n",
    "    hmm_model.fit(X_train)\n",
    "    print(\"Model successfully trained with Baum-Welch algorithm.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# TEST FEATURE PREDICTION PART (using DCT features)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Function to predict character sequences using Viterbi algorithm\n",
    "# The Viterbi algorithm is used here for decoding, providing the most likely sequence of hidden states\n",
    "def predict_from_features(test_features):\n",
    "    if test_features.shape[0] != X.shape[1]:  # Ensure correct number of features\n",
    "        raise ValueError(f\"Expected {X.shape[1]} features, but got {test_features.shape[0]}\")\n",
    "\n",
    "    # Reshape test features to match HMM input requirements\n",
    "    test_features = test_features.reshape(1, -1)\n",
    "\n",
    "    # Viterbi algorithm for decoding the most likely hidden state sequence\n",
    "    # This outputs the sequence of states most likely responsible for the observed sequence\n",
    "    try:\n",
    "        predicted_states = hmm_model.predict(test_features)\n",
    "        predicted_sequence = ''.join(int_to_char[state] for state in predicted_states if state != -1)\n",
    "        print(\"Predicted sequence for the test features:\", predicted_sequence)\n",
    "    except ValueError as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "\n",
    "\n",
    "    print(\"Predicted states:\", predicted_states)\n",
    "\n",
    "    predicted_sequence = ''.join(int_to_char[state] for state in predicted_states if state in int_to_char)\n",
    "\n",
    "# Example: Use random test features (ensure it has the same length as X's feature dimension)\n",
    "# Replace `test_features` with actual data for real predictions\n",
    "test_features = np.random.rand(X.shape[1])  # Random example for testing purposes\n",
    "\n",
    "# Run prediction function to obtain character sequence from test features\n",
    "predict_from_features(test_features)\n",
    "\n",
    "\n",
    "print(\"State to character mapping:\", int_to_char)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sc0Sa4brO5X0",
    "outputId": "099e1a3a-eff7-4baa-9e5b-89acdbe9ab86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:hmmlearn.base:Model is not converging.  Current: -6001.277196270626 is not greater than -6001.277189075525. Delta is -7.195100806711707e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features file columns: Index(['Unnamed: 0',            0,            1,            2,            3,\n",
      "                  4,            5,            6,            7,            8,\n",
      "       ...\n",
      "                192,          193,          194,          195,          196,\n",
      "                197,          198,          199,        '0.1', 'image_name'],\n",
      "      dtype='object', length=203)\n",
      "Labels file columns: Index(['gt', 'image_name'], dtype='object')\n",
      "Model successfully trained with Baum-Welch algorithm.\n",
      "Predicted sequence for the test sequence: അഏഎ ം:ം::ം\n",
      "Predicted states: [ 5 10  9  2  3  1  3  1  1  3]\n",
      "State to character mapping: {0: ' ', 1: ':', 2: '\\xa0', 3: 'ം', 4: 'ഃ', 5: 'അ', 6: 'ആ', 7: 'ഇ', 8: 'ഉ', 9: 'എ', 10: 'ഏ', 11: 'ഒ', 12: 'ഓ', 13: 'ക', 14: 'ഖ', 15: 'ഗ', 16: 'ഘ', 17: 'ങ', 18: 'ച', 19: 'ഛ', 20: 'ജ', 21: 'ഝ', 22: 'ഞ', 23: 'ട', 24: 'ഠ', 25: 'ഡ', 26: 'ഢ', 27: 'ണ', 28: 'ത', 29: 'ഥ', 30: 'ദ', 31: 'ധ', 32: 'ന', 33: 'പ', 34: 'ഫ', 35: 'ബ', 36: 'ഭ', 37: 'മ', 38: 'യ', 39: 'ര', 40: 'റ', 41: 'ല', 42: 'ള', 43: 'ഴ', 44: 'വ', 45: 'ശ', 46: 'ഷ', 47: 'സ', 48: 'ഹ', 49: 'ാ', 50: 'ി', 51: 'ീ', 52: 'ു', 53: 'ൂ', 54: 'ൃ', 55: 'െ', 56: 'േ', 57: 'ൈ', 58: 'ൊ', 59: 'ോ', 60: '്', 61: 'ൗ', 62: 'ൻ', 63: 'ർ', 64: 'ൽ', 65: 'ൾ', 66: '\\u200c', 67: '\\u200d'}\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install pandas numpy scikit-learn hmmlearn opencv-python matplotlib openpyxl\n",
    "\n",
    "# Required libraries for handling data and modeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Specify file names for easy reference\n",
    "feature_file = '/content/drive/MyDrive/Colab Notebooks/SFR_DCT_LINE 2.xlsx'  # DCT features file (uploaded manually)\n",
    "label_file = '/content/drive/MyDrive/Colab Notebooks/line_gt_1_1.xlsx'        # Label file (uploaded manually)\n",
    "\n",
    "\n",
    "# Step 1: Load the feature and label data\n",
    "features = pd.read_excel(feature_file)\n",
    "labels = pd.read_excel(label_file)\n",
    "\n",
    "# Display column names for both datasets\n",
    "print(\"Features file columns:\", features.columns)\n",
    "print(\"Labels file columns:\", labels.columns)\n",
    "\n",
    "\n",
    "# Step 2: Merge features and labels on 'image_name' for consistent mapping\n",
    "merged_data = pd.merge(features, labels, on='image_name')\n",
    "merged_data.fillna(0, inplace=True)  # Fill NaN values with zeros\n",
    "\n",
    "# Step 3: Preprocess the data for HMM training\n",
    "# Drop 'image_name' and 'gt' columns to get only feature values\n",
    "X = merged_data.drop(columns=['image_name', 'gt']).select_dtypes(include=[np.number]).values\n",
    "\n",
    "# Scale features to mean 0 and variance 1 (to avoid numerical issues in HMM)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Map unique characters in 'gt' to integers for training\n",
    "unique_chars = sorted(set(''.join(merged_data['gt'].values)))\n",
    "char_to_int = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "int_to_char = {idx: char for char, idx in char_to_int.items()}\n",
    "\n",
    "# Convert 'gt' labels into sequences of integers for training\n",
    "y_sequences = [[char_to_int[char] for char in label] for label in merged_data['gt'].values]\n",
    "\n",
    "# Pad sequences to ensure consistent sequence lengths (fill with -1 for padding)\n",
    "max_sequence_length = max(len(seq) for seq in y_sequences)\n",
    "y_padded = np.array([seq + [-1] * (max_sequence_length - len(seq)) for seq in y_sequences])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y_padded[:train_size], y_padded[train_size:]\n",
    "\n",
    "# Step 6: Initialize and Train the HMM model using Baum-Welch algorithm\n",
    "n_states = 20  # Number of hidden states\n",
    "hmm_model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"spherical\", n_iter=2000, init_params='')\n",
    "\n",
    "# Set start and transition probabilities uniformly\n",
    "hmm_model.startprob_ = np.full(n_states, 1.0 / n_states)\n",
    "hmm_model.transmat_ = np.full((n_states, n_states), 1.0 / n_states)\n",
    "\n",
    "# Train the HMM model\n",
    "try:\n",
    "    hmm_model.fit(X_train)\n",
    "    print(\"Model successfully trained with Baum-Welch algorithm.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Prediction Function for Character Sequences\n",
    "# -------------------------------------------------------\n",
    "def predict_from_sequence(test_sequence):\n",
    "    if test_sequence.shape[1] != X.shape[1]:  # Ensure the feature dimension matches\n",
    "        raise ValueError(f\"Expected {X.shape[1]} features, but got {test_sequence.shape[1]}\")\n",
    "\n",
    "    # Predict the sequence of states for multiple samples\n",
    "    try:\n",
    "        predicted_states = hmm_model.predict(test_sequence)\n",
    "        predicted_sequence = ''.join(int_to_char[state] for state in predicted_states if state in int_to_char)\n",
    "\n",
    "        # Display results\n",
    "        print(\"Predicted sequence for the test sequence:\", predicted_sequence)\n",
    "        print(\"Predicted states:\", predicted_states)\n",
    "    except ValueError as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "\n",
    "# Run the prediction with a test sequence (e.g., first 10 samples of X_test)\n",
    "test_sequence = X_test[:10]  # Using multiple rows to form a sequence\n",
    "\n",
    "# Predict and display results\n",
    "predict_from_sequence(test_sequence)\n",
    "\n",
    "# Display state-to-character mapping for reference\n",
    "print(\"State to character mapping:\", int_to_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_fVexavdmlv",
    "outputId": "ea699c43-800a-4aae-9be6-7a92f413b3a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Features file columns: Index(['Unnamed: 0',            0,            1,            2,            3,\n",
      "                  4,            5,            6,            7,            8,\n",
      "       ...\n",
      "                192,          193,          194,          195,          196,\n",
      "                197,          198,          199,        '0.1', 'image_name'],\n",
      "      dtype='object', length=203)\n",
      "Labels file columns: Index(['gt', 'image_name'], dtype='object')\n",
      "Model successfully trained with Baum-Welch algorithm.\n",
      "Predicted sequence for the test sequence: ഒംഘക ഛഎഅഖഖഖഖഖഖ:ഃ\n",
      "Predicted states: [11  3 16 13  2 19  9  5 14 14 14 14 14 14  1  4]\n",
      "State to character mapping: {0: ' ', 1: ':', 2: '\\xa0', 3: 'ം', 4: 'ഃ', 5: 'അ', 6: 'ആ', 7: 'ഇ', 8: 'ഉ', 9: 'എ', 10: 'ഏ', 11: 'ഒ', 12: 'ഓ', 13: 'ക', 14: 'ഖ', 15: 'ഗ', 16: 'ഘ', 17: 'ങ', 18: 'ച', 19: 'ഛ', 20: 'ജ', 21: 'ഝ', 22: 'ഞ', 23: 'ട', 24: 'ഠ', 25: 'ഡ', 26: 'ഢ', 27: 'ണ', 28: 'ത', 29: 'ഥ', 30: 'ദ', 31: 'ധ', 32: 'ന', 33: 'പ', 34: 'ഫ', 35: 'ബ', 36: 'ഭ', 37: 'മ', 38: 'യ', 39: 'ര', 40: 'റ', 41: 'ല', 42: 'ള', 43: 'ഴ', 44: 'വ', 45: 'ശ', 46: 'ഷ', 47: 'സ', 48: 'ഹ', 49: 'ാ', 50: 'ി', 51: 'ീ', 52: 'ു', 53: 'ൂ', 54: 'ൃ', 55: 'െ', 56: 'േ', 57: 'ൈ', 58: 'ൊ', 59: 'ോ', 60: '്', 61: 'ൗ', 62: 'ൻ', 63: 'ർ', 64: 'ൽ', 65: 'ൾ', 66: '\\u200c', 67: '\\u200d'}\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install pandas numpy scikit-learn hmmlearn opencv-python matplotlib openpyxl\n",
    "\n",
    "# Required libraries for handling data and modeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Specify file names for easy reference\n",
    "feature_file = '/content/drive/MyDrive/Colab Notebooks/SFR_DCT_LINE 2.xlsx'  # DCT features file (uploaded manually)\n",
    "label_file = '/content/drive/MyDrive/Colab Notebooks/line_gt_1_1.xlsx'        # Label file (uploaded manually)\n",
    "\n",
    "\n",
    "# Step 1: Load the feature and label data\n",
    "features = pd.read_excel(feature_file)\n",
    "labels = pd.read_excel(label_file)\n",
    "\n",
    "# Display column names for both datasets\n",
    "print(\"Features file columns:\", features.columns)\n",
    "print(\"Labels file columns:\", labels.columns)\n",
    "\n",
    "\n",
    "# Step 2: Merge features and labels on 'image_name' for consistent mapping\n",
    "merged_data = pd.merge(features, labels, on='image_name')\n",
    "merged_data.fillna(0, inplace=True)  # Fill NaN values with zeros\n",
    "\n",
    "# Step 3: Preprocess the data for HMM training\n",
    "# Drop 'image_name' and 'gt' columns to get only feature values\n",
    "X = merged_data.drop(columns=['image_name', 'gt']).select_dtypes(include=[np.number]).values\n",
    "\n",
    "# Scale features to mean 0 and variance 1 (to avoid numerical issues in HMM)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Map unique characters in 'gt' to integers for training\n",
    "unique_chars = sorted(set(''.join(merged_data['gt'].values)))\n",
    "char_to_int = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "int_to_char = {idx: char for char, idx in char_to_int.items()}\n",
    "\n",
    "# Convert 'gt' labels into sequences of integers for training\n",
    "y_sequences = [[char_to_int[char] for char in label] for label in merged_data['gt'].values]\n",
    "\n",
    "# Pad sequences to ensure consistent sequence lengths (fill with -1 for padding)\n",
    "max_sequence_length = max(len(seq) for seq in y_sequences)\n",
    "y_padded = np.array([seq + [-1] * (max_sequence_length - len(seq)) for seq in y_sequences])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y_padded[:train_size], y_padded[train_size:]\n",
    "\n",
    "# Step 6: Initialize and Train the HMM model using Baum-Welch algorithm\n",
    "n_states = 20  # Number of hidden states\n",
    "hmm_model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"spherical\", n_iter=2000, init_params='')\n",
    "\n",
    "# Set start and transition probabilities uniformly\n",
    "hmm_model.startprob_ = np.full(n_states, 1.0 / n_states)\n",
    "hmm_model.transmat_ = np.full((n_states, n_states), 1.0 / n_states)\n",
    "\n",
    "# Train the HMM model\n",
    "try:\n",
    "    hmm_model.fit(X_train)\n",
    "    print(\"Model successfully trained with Baum-Welch algorithm.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Prediction Function for Character Sequences\n",
    "# -------------------------------------------------------\n",
    "def predict_from_sequence(test_sequence):\n",
    "    if test_sequence.shape[1] != X.shape[1]:  # Ensure the feature dimension matches\n",
    "        raise ValueError(f\"Expected {X.shape[1]} features, but got {test_sequence.shape[1]}\")\n",
    "\n",
    "    # Predict the sequence of states for multiple samples\n",
    "    try:\n",
    "        predicted_states = hmm_model.predict(test_sequence)\n",
    "        predicted_sequence = ''.join(int_to_char[state] for state in predicted_states if state in int_to_char)\n",
    "\n",
    "        # Display results\n",
    "        print(\"Predicted sequence for the test sequence:\", predicted_sequence)\n",
    "        print(\"Predicted states:\", predicted_states)\n",
    "    except ValueError as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "\n",
    "# Run the prediction with a test sequence (e.g., first 10 samples of X_test)\n",
    "test_sequence = X_test[:20]  # Using multiple rows to form a sequence\n",
    "\n",
    "# Predict and display results\n",
    "predict_from_sequence(test_sequence)\n",
    "\n",
    "# Display state-to-character mapping for reference\n",
    "print(\"State to character mapping:\", int_to_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "id": "Mbpc1qsKyLjh",
    "outputId": "e1dd19c2-4bb2-43a2-a5cf-b7d441e7543f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8e4d2ae2262e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_sequence' is not defined"
     ]
    }
   ],
   "source": [
    "print(test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uHtHAnTfNrm",
    "outputId": "4777d753-df8e-4505-e539-05107e4f862b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Collecting hmmlearn\n",
      "  Downloading hmmlearn-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading hmmlearn-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.1/161.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hmmlearn\n",
      "Successfully installed hmmlearn-0.3.2\n",
      "Mounted at /content/drive\n",
      "Features file columns: Index(['Unnamed: 0',            0,            1,            2,            3,\n",
      "                  4,            5,            6,            7,            8,\n",
      "       ...\n",
      "                192,          193,          194,          195,          196,\n",
      "                197,          198,          199,        '0.1', 'image_name'],\n",
      "      dtype='object', length=203)\n",
      "Labels file columns: Index(['gt', 'image_name'], dtype='object')\n",
      "Model successfully trained with Baum-Welch algorithm.\n",
      "Predicted sequence for the test sequence: ഘംക:എഅഅഅഅഅഅഅഅഅഅഅ\n",
      "Predicted states: [16  3 13  1  9  5  5  5  5  5  5  5  5  5  5  5]\n",
      "State to character mapping: {0: ' ', 1: ':', 2: '\\xa0', 3: 'ം', 4: 'ഃ', 5: 'അ', 6: 'ആ', 7: 'ഇ', 8: 'ഉ', 9: 'എ', 10: 'ഏ', 11: 'ഒ', 12: 'ഓ', 13: 'ക', 14: 'ഖ', 15: 'ഗ', 16: 'ഘ', 17: 'ങ', 18: 'ച', 19: 'ഛ', 20: 'ജ', 21: 'ഝ', 22: 'ഞ', 23: 'ട', 24: 'ഠ', 25: 'ഡ', 26: 'ഢ', 27: 'ണ', 28: 'ത', 29: 'ഥ', 30: 'ദ', 31: 'ധ', 32: 'ന', 33: 'പ', 34: 'ഫ', 35: 'ബ', 36: 'ഭ', 37: 'മ', 38: 'യ', 39: 'ര', 40: 'റ', 41: 'ല', 42: 'ള', 43: 'ഴ', 44: 'വ', 45: 'ശ', 46: 'ഷ', 47: 'സ', 48: 'ഹ', 49: 'ാ', 50: 'ി', 51: 'ീ', 52: 'ു', 53: 'ൂ', 54: 'ൃ', 55: 'െ', 56: 'േ', 57: 'ൈ', 58: 'ൊ', 59: 'ോ', 60: '്', 61: 'ൗ', 62: 'ൻ', 63: 'ർ', 64: 'ൽ', 65: 'ൾ', 66: '\\u200c', 67: '\\u200d'}\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install pandas numpy scikit-learn hmmlearn opencv-python matplotlib openpyxl\n",
    "\n",
    "# Required libraries for handling data and modeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Specify file names for easy reference\n",
    "feature_file = '/content/drive/MyDrive/Colab Notebooks/SFR_DCT_LINE 2.xlsx'  # DCT features file (uploaded manually)\n",
    "label_file = '/content/drive/MyDrive/Colab Notebooks/line_gt_1_1.xlsx'        # Label file (uploaded manually)\n",
    "\n",
    "\n",
    "# Step 1: Load the feature and label data\n",
    "features = pd.read_excel(feature_file)\n",
    "labels = pd.read_excel(label_file)\n",
    "\n",
    "# Display column names for both datasets\n",
    "print(\"Features file columns:\", features.columns)\n",
    "print(\"Labels file columns:\", labels.columns)\n",
    "\n",
    "\n",
    "# Step 2: Merge features and labels on 'image_name' for consistent mapping\n",
    "merged_data = pd.merge(features, labels, on='image_name')\n",
    "merged_data.fillna(0, inplace=True)  # Fill NaN values with zeros\n",
    "\n",
    "# Step 3: Preprocess the data for HMM training\n",
    "# Drop 'image_name' and 'gt' columns to get only feature values\n",
    "X = merged_data.drop(columns=['image_name', 'gt']).select_dtypes(include=[np.number]).values\n",
    "\n",
    "# Scale features to mean 0 and variance 1 (to avoid numerical issues in HMM)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Map unique characters in 'gt' to integers for training\n",
    "unique_chars = sorted(set(''.join(merged_data['gt'].values)))\n",
    "char_to_int = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "int_to_char = {idx: char for char, idx in char_to_int.items()}\n",
    "\n",
    "# Convert 'gt' labels into sequences of integers for training\n",
    "y_sequences = [[char_to_int[char] for char in label] for label in merged_data['gt'].values]\n",
    "\n",
    "# Pad sequences to ensure consistent sequence lengths (fill with -1 for padding)\n",
    "max_sequence_length = max(len(seq) for seq in y_sequences)\n",
    "y_padded = np.array([seq + [-1] * (max_sequence_length - len(seq)) for seq in y_sequences])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y_padded[:train_size], y_padded[train_size:]\n",
    "\n",
    "# Step 6: Initialize and Train the HMM model using Baum-Welch algorithm\n",
    "n_states = 20  # Number of hidden states\n",
    "hmm_model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"spherical\", n_iter=2000, init_params='')\n",
    "\n",
    "# Set start and transition probabilities uniformly\n",
    "hmm_model.startprob_ = np.full(n_states, 1.0 / n_states)\n",
    "hmm_model.transmat_ = np.full((n_states, n_states), 1.0 / n_states)\n",
    "\n",
    "# Train the HMM model\n",
    "try:\n",
    "    hmm_model.fit(X_train)\n",
    "    print(\"Model successfully trained with Baum-Welch algorithm.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Prediction Function for Character Sequences\n",
    "# -------------------------------------------------------\n",
    "def predict_from_sequence(test_sequence):\n",
    "    if test_sequence.shape[1] != X.shape[1]:  # Ensure the feature dimension matches\n",
    "        raise ValueError(f\"Expected {X.shape[1]} features, but got {test_sequence.shape[1]}\")\n",
    "\n",
    "    # Predict the sequence of states for multiple samples\n",
    "    try:\n",
    "        predicted_states = hmm_model.predict(test_sequence)\n",
    "        predicted_sequence = ''.join(int_to_char[state] for state in predicted_states if state in int_to_char)\n",
    "\n",
    "        # Display results\n",
    "        print(\"Predicted sequence for the test sequence:\", predicted_sequence)\n",
    "        print(\"Predicted states:\", predicted_states)\n",
    "    except ValueError as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "\n",
    "# Run the prediction with a test sequence (e.g., first 10 samples of X_test)\n",
    "test_sequence = X_test[:20]  # Using multiple rows to form a sequence\n",
    "\n",
    "# Predict and display results\n",
    "predict_from_sequence(test_sequence)\n",
    "\n",
    "# Display state-to-character mapping for reference\n",
    "print(\"State to character mapping:\", int_to_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AT0S_5EtROIf",
    "outputId": "ca5de6d7-c9c6-4bad-866b-5d776c4e56e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
      "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:hmmlearn.base:Model is not converging.  Current: -6542.001678946999 is not greater than -6542.001676867487. Delta is -2.0795123418793082e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features file columns: Index(['Unnamed: 0',            0,            1,            2,            3,\n",
      "                  4,            5,            6,            7,            8,\n",
      "       ...\n",
      "                192,          193,          194,          195,          196,\n",
      "                197,          198,          199,        '0.1', 'image_name'],\n",
      "      dtype='object', length=203)\n",
      "Labels file columns: Index(['gt', 'image_name'], dtype='object')\n",
      "Model successfully trained with Baum-Welch algorithm.\n",
      "Predicted sequence for the test row: ഘഇഉആഃഛഎഎഎഎഎഎഎഎംംംഎഎംഎഎഎഎഎംഎഎംംഎഎഎഎഎംഎഎഎഎഎഎഎഎഎംംഎംഎഎംംഎഎഎഎഎംംഎഎഎഎഎഎംംഎഎഎഎഎംഎഎംഎഎഎഎഎഎഎഎഎഎഎഎഎഎംംഎഎഎഎഎഎഎഎഎഎഎംഎഎഎഎഎഎഎഎഎംഎഎഎഎഎംംഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎംഎഎഎഎംഎഎഎഎഎഎംംഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎഎംഎഎഎഎംംഎഎഎഎഎഎഎഎം\n",
      "Predicted states: [16  7  8  6  4 19  9  9  9  9  9  9  9  9  3  3  3  9  9  3  9  9  9  9\n",
      "  9  3  9  9  3  3  9  9  9  9  9  3  9  9  9  9  9  9  9  9  9  3  3  9\n",
      "  3  9  9  3  3  9  9  9  9  9  3  3  9  9  9  9  9  9  3  3  9  9  9  9\n",
      "  9  3  9  9  3  9  9  9  9  9  9  9  9  9  9  9  9  9  9  3  3  9  9  9\n",
      "  9  9  9  9  9  9  9  9  3  9  9  9  9  9  9  9  9  9  3  9  9  9  9  9\n",
      "  3  3  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  3  9  9\n",
      "  9  9  3  9  9  9  9  9  9  3  3  9  9  9  9  9  9  9  9  9  9  9  9  9\n",
      "  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  3  9  9  9  9  3  3\n",
      "  9  9  9  9  9  9  9  9  3]\n",
      "State to character mapping: {0: ' ', 1: ':', 2: '\\xa0', 3: 'ം', 4: 'ഃ', 5: 'അ', 6: 'ആ', 7: 'ഇ', 8: 'ഉ', 9: 'എ', 10: 'ഏ', 11: 'ഒ', 12: 'ഓ', 13: 'ക', 14: 'ഖ', 15: 'ഗ', 16: 'ഘ', 17: 'ങ', 18: 'ച', 19: 'ഛ', 20: 'ജ', 21: 'ഝ', 22: 'ഞ', 23: 'ട', 24: 'ഠ', 25: 'ഡ', 26: 'ഢ', 27: 'ണ', 28: 'ത', 29: 'ഥ', 30: 'ദ', 31: 'ധ', 32: 'ന', 33: 'പ', 34: 'ഫ', 35: 'ബ', 36: 'ഭ', 37: 'മ', 38: 'യ', 39: 'ര', 40: 'റ', 41: 'ല', 42: 'ള', 43: 'ഴ', 44: 'വ', 45: 'ശ', 46: 'ഷ', 47: 'സ', 48: 'ഹ', 49: 'ാ', 50: 'ി', 51: 'ീ', 52: 'ു', 53: 'ൂ', 54: 'ൃ', 55: 'െ', 56: 'േ', 57: 'ൈ', 58: 'ൊ', 59: 'ോ', 60: '്', 61: 'ൗ', 62: 'ൻ', 63: 'ർ', 64: 'ൽ', 65: 'ൾ', 66: '\\u200c', 67: '\\u200d'}\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install pandas numpy scikit-learn hmmlearn opencv-python matplotlib openpyxl\n",
    "\n",
    "# Required libraries for handling data and modeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Specify file names for easy reference\n",
    "feature_file = '/content/drive/MyDrive/Colab Notebooks/SFR_DCT_LINE 2.xlsx'  # DCT features file (uploaded manually)\n",
    "label_file = '/content/drive/MyDrive/Colab Notebooks/line_gt_1_1.xlsx'        # Label file (uploaded manually)\n",
    "\n",
    "\n",
    "# Step 1: Load the feature and label data\n",
    "features = pd.read_excel(feature_file)\n",
    "labels = pd.read_excel(label_file)\n",
    "\n",
    "# Display column names for both datasets\n",
    "print(\"Features file columns:\", features.columns)\n",
    "print(\"Labels file columns:\", labels.columns)\n",
    "\n",
    "\n",
    "# Step 2: Merge features and labels on 'image_name' for consistent mapping\n",
    "merged_data = pd.merge(features, labels, on='image_name')\n",
    "merged_data.fillna(0, inplace=True)  # Fill NaN values with zeros\n",
    "\n",
    "# Step 3: Preprocess the data for HMM training\n",
    "# Drop 'image_name' and 'gt' columns to get only feature values\n",
    "X = merged_data.drop(columns=['image_name', 'gt']).select_dtypes(include=[np.number]).values\n",
    "\n",
    "# Scale features to mean 0 and variance 1 (to avoid numerical issues in HMM)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Map unique characters in 'gt' to integers for training\n",
    "unique_chars = sorted(set(''.join(merged_data['gt'].values)))\n",
    "char_to_int = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "int_to_char = {idx: char for char, idx in char_to_int.items()}\n",
    "\n",
    "# Convert 'gt' labels into sequences of integers for training\n",
    "y_sequences = [[char_to_int[char] for char in label] for label in merged_data['gt'].values]\n",
    "\n",
    "# Pad sequences to ensure consistent sequence lengths (fill with -1 for padding)\n",
    "max_sequence_length = max(len(seq) for seq in y_sequences)\n",
    "y_padded = np.array([seq + [-1] * (max_sequence_length - len(seq)) for seq in y_sequences])\n",
    "\n",
    "# Step 5: Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y_padded[:train_size], y_padded[train_size:]\n",
    "\n",
    "# Step 6: Initialize and Train the HMM model using Baum-Welch algorithm\n",
    "n_states = 20  # Number of hidden states\n",
    "hmm_model = hmm.GaussianHMM(n_components=n_states, covariance_type=\"spherical\", n_iter=2000, init_params='')\n",
    "\n",
    "# Set start and transition probabilities uniformly\n",
    "hmm_model.startprob_ = np.full(n_states, 1.0 / n_states)\n",
    "hmm_model.transmat_ = np.full((n_states, n_states), 1.0 / n_states)\n",
    "\n",
    "# Train the HMM model\n",
    "try:\n",
    "    hmm_model.fit(X_train)\n",
    "    print(\"Model successfully trained with Baum-Welch algorithm.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Adjusted Prediction Function for Full Sequence Row with 201 Timesteps\n",
    "# -------------------------------------------------------\n",
    "def predict_full_sequence(test_row):\n",
    "    # Reshape each row into 201 timesteps with 1 feature each\n",
    "    n_timesteps = 201  # Matches the number of features in each row\n",
    "    test_sequence = test_row.reshape(n_timesteps, 1)\n",
    "\n",
    "    # Predict the sequence of states for each timestep in the test sequence\n",
    "    try:\n",
    "        predicted_states = hmm_model.predict(test_sequence)\n",
    "\n",
    "        # Map each predicted state to its character to form a full sequence\n",
    "        predicted_sequence = ''.join(int_to_char[state] for state in predicted_states if state in int_to_char)\n",
    "\n",
    "        # Display results\n",
    "        print(\"Predicted sequence for the test row:\", predicted_sequence)\n",
    "        print(\"Predicted states:\", predicted_states)\n",
    "    except ValueError as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "\n",
    "# Run the prediction with a single row (e.g., the first sample of X_test)\n",
    "test_row = X_test[2]  # Using one row as the test feature\n",
    "\n",
    "# Predict and display results\n",
    "predict_full_sequence(test_row)\n",
    "\n",
    "# Display state-to-character mapping for reference\n",
    "print(\"State to character mapping:\", int_to_char)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
